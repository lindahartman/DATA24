[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "Welcome to Data Literacy with Python!\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. To keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about crunching numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nData analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\n\nWhich part of the data should you focus on?\nWhat variables should you use?\nWhich patterns caught your eye?\n\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work. And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nData analysis is evolving. Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "index.html#video-1-studio-12-min",
    "href": "index.html#video-1-studio-12-min",
    "title": "Data Literacy with Python",
    "section": "",
    "text": "Welcome to Data Literacy with Python!\nLet me ask you something: Can you imagine living in today’s world but being unable to read? Think about it—street signs wouldn’t make sense, advertisements would just be noise, and most of the internet? Completely out of reach.\nNow, even with videos and voice assistants everywhere, written text is still the backbone of how we communicate and navigate life. Without it, you’d feel lost.\nBut here’s the thing: today’s world doesn’t just run on words. It runs on data.\nEvery day, we’re creating over 400 million terabytes of data. That’s every single day. And here’s a wild stat—90% of all the world’s data was created in just the last two years.\nThis explosion of information is transforming how we make decisions, whether it’s in business, science, or society as a whole. To keep up, you need to know how to make sense of it.\nData isn’t just numbers on a screen—it’s stories waiting to be uncovered. And understanding data has become just as important as being able to read or write.\nThat’s where this course comes in.\nWe’re going to teach you how to take raw, messy data and turn it into something meaningful. You’ll work with rectangular data—the kind you find in spreadsheets or databases.\nAnd don’t worry—this isn’t just about crunching numbers. It’s about answering real-world questions, solving problems, and making decisions based on insights you uncover.\nBy the end of this course, you’ll have the skills to transform data into knowledge.\nLet’s talk about the tools you need to work with data.\nYou might be tempted by low-code or no-code solutions—those point-and-click interfaces that make everything seem so easy. And sure, they’re great for quick wins. But when it comes to serious data analysis, they have some big limitations.\nData analysis isn’t just about getting answers—it’s about getting credible answers.\nTo trust your insights, you need to leave a trail. Think about it—during analysis, you make dozens of tiny decisions:\n\nWhich part of the data should you focus on?\nWhat variables should you use?\nWhich patterns caught your eye?\n\nEvery decision shapes your results. And if you—or anyone else—can’t retrace those steps, how can you be sure your conclusions hold up?\nThat’s why scripting your analysis is so important.\nWith a script, every step is recorded. You can spot mistakes, refine your work, or pick up right where you left off—even months later. Low-code tools? They don’t give you that kind of transparency.\nSo, what’s the best language for scripting your data analysis?\nThe answer is Python.\nPython is the world’s most popular programming language, and for good reason. Created in 1990 by Guido van Rossum, Python has become the go-to language for everything from building websites to powering cutting-edge AI. It may not be the fastest language out there, but it’s arguably the most readable. And in today’s data-driven world, readability matters more than ever.\nThe Python ecosystem for data analysis is enormous. Whatever your question, there’s a good chance Python has a library—or ten—that can help.\nData analysis is unique—it’s less about traditional programming and more about crafting a story with your data. Your code should be clear and intuitive, not just for you, but for anyone who needs to understand your work. And that includes “future you”—because six months from now, you might not even recognize your own analysis without clear documentation!\nSo, as we dive into this course, we’ll emphasize simplicity, transparency, and readability. Because great analysis isn’t just about crunching numbers—it’s about telling a story that stands the test of time.\nData analysis is evolving. Today, some of the most cutting-edge tools are built on high-performance programming languages like Rust, Java, or C++. Why? Because these languages are fast—lightning fast. But here’s the best part: you don’t need to write in these languages to enjoy their benefits.\nModern tools now separate the user interface from the engine. That means the algorithms working behind the scenes are the same, no matter which scripting language you use.\nInitiatives like Apache Arrow go even further—they create standardized data formats, making it easy to move between tools and platforms without losing performance or compatibility.\nIn this course, we’re diving into tools built on Rust—one of the fastest, most efficient programming languages out there. Specifically, we’ll use uv for managing packages and environments and polars for data wrangling.\nThese tools are not just fast—they’re scalable.\nThe examples we’ll explore together are small—easy to follow and understand. But don’t let that fool you. The same tools we use here can scale effortlessly to handle datasets with billions of rows, processed across dozens of parallel machines.\nWhat’s even better? The interface doesn’t change.\nSo whether you’re working on a personal project, academic research, or a large-scale business application, the skills you gain here will translate directly to the real world.\nThe datasets may be small, but the questions and challenges we tackle are universal. By the end of this course, you’ll be equipped to uncover meaningful insights from your own data, no matter its size or complexity.\nLet’s get started on this exciting journey into the world of data literacy!"
  },
  {
    "objectID": "wrangling.html",
    "href": "wrangling.html",
    "title": "Wrangling",
    "section": "",
    "text": "VIDEO 1: DATA {greenscreen} ~ 4 min\n\nWelcome to the module on data wrangling! In the last lesson, we explored a small, but exciting gapminder dataset, and created quite a few visualizations with it. But in real-world scenarios, datasets are often much larger. This brings new challenges, like focusing on specific subsets of data-perhaps observations from a single time period or a selection of variables related to a particular phenomenon.\nIn this module, we’ll learn how to subset data and create meaningful summaries that provide a high-level overview of trends or differences between groups. Summarized data is often presented in tables, so we’ll also introduce a package for creating clear, professional-looking tables.\nMost importantly, we’ll dive into the blazingly fast Polars package for data manipulation. As I mentioned earlier, Polars is powered by Rust, a high-performance programming language. Its core functionality is exposed to Python but can also be accessed from other languages. This means the data wrangling skills you gain here will be transferable beyond Python.\nBut first, let’s load the necessary packages for this module. In Python, it’s common to use the alias pl for Polars. We’ll also use a submodule called polars.selectors, aliasing it as cs - don’t worry, we’ll cover selectors in more detail soon. We’ll also import everything from Plotnine for data visualization and bring in the main function from the great_tables package for generating beautifully looking tables.\nHere’s the setup code. Place it in its own cell in your notebook and run it:\n&lt; PAUSE &gt;\nNow, let’s explore our data. We’ll be looking at the WHO dataset about household pollution around the world.\nHousehold air pollution is primarily caused by burning polluting fuels like wood, animal dung, charcoal, agricultural waste, and kerosene using open fires or inefficient stoves. Over 2 billion of people around the world rely on these fuels for cooking, heating, and lighting. The poor combustion of these fuels leads to numerous health issues, such as pneumonia in children, and chronic diseases like obstructive pulmonary disease, lung cancer, stroke, and cardiovascular problems in adults.\nThis is a complex problem, and like many complex problems, it can be examined from different angles. We have three datasets to work with:\n\nCauses of death linked to household pollution\nTypes of fuel used for cooking in various countries\nProportion of the population with access to clean cooking fuels\n\nEach dataset offers a unique perspective on this issue. Let’s dive in and start exploring!\n\n\nVIDEO 2: IMPORT {greenscreen} ~ 12 min\n\nIn this section, we’ll start working with the datasets about household air pollution. These datasets are stored as comma-separated values, or CSV files. CSV is a simple text format often used for storing tabular data. Think of it as a stripped-down version of Excel - just the data, no formatting or formulas. In fact, you can even open CSV files directly in Excel, if you want to take a look at them.\nWe’ve prepared these datasets for you and stored them in the course repository on GitHub. While Polars allows us to read files directly from remote locations, you can also download the files and the load them into Python from your local project directory.\nHere’s the code to read the three datasets we’ll use. Add these lines to a new cell in your notebook and run it.\n&lt; PAUSE &gt;\n\nLet’s break this down. We’re using the read_csv() function from Polars to load the data. This function takes a single mandatory argument: the file path, written as a string in quotes. The equal sign indicates that we assign the content of the file to the variable, listed on the left. The function is located in the polars package. This is why it is prepended with pl. prefix.\nThe read_csv() function returns a data frame. So, from now on, we can simply refer to h-hap underscore deaths (hhap_deaths) whenever we need to access the first dataframe without needing to re-import it.\n\nTo understand the data we’re working with, it’s helpful to preview it in a few ways. For instance, simply typing the name of a dataset - like clean_fuels - will display a preview of the first five and last five rows of the corresponding data frame.\nRows in a data frame are often referred to as observations or records, while columns are known as variables or features. If you want to see more rows than the default preview, you can use the .head() method and specify the number of rows to display:\n&lt; PAUSE &gt;\nYou may have noticed the parentheses around the code block. This lets you write the code across multiple lines without worrying about indentation. To make the code more readable I will place new methods on a new line. In conventional Python coding style this would not be allowed, because indentation in Python matters. But because we wrap our code into outer patherenthesis, Python will overlook the arbitrary formating of code. In this case we prioritize readability over the standard coding style.\nPreviewing the first few rows gives you an initial sense of the dataset’s structure and content. If you’re curious about the last few rows, there’s also a .tail() method you can use in a similar way.\nFor a broader overview of your data, you can use the .describe() method:\n&lt; PAUSE &gt;\nThe .describe() method provides a statistical summary of numerical columns, including metrics like the mean, standard deviation, minimum, maximum, and various quantiles. It’s especially useful for large datasets when you want to quickly understand key metrics.\nOne thing to note: if a column contains missing values, Polars will display these as null. Polars treats missing values as “contagious,” so any operation involving them will also result in missing values in the output. This behavior applies to all statistical operations. We’ll see more examples of this later.\nBoth .head() and .describe() return a data frame, which means you can chain these operations together. Method chaining is a powerful coding style that helps you write clean, readable, and maintainable code.\nHere are a couple of challenges for you to practice combining the functions into a method chain: Take first 25 records of clean_fuels data frame and calculate statistical summary. When you are done, compute statistical summary of the whole dataset and then present only quantile summaries of each column. The quantiles include minimum, maximum, as well as the 25th, 50th and 75th quantiles.\n\n&lt; PAUSE &gt;\nSometimes, datasets have many columns, making it difficult to gain a full overview using methods like head() or describe(). For these cases, Polars provides a particularly useful method called glimpse().\nWhen you glimpse(), the dataset’s structure is displayed horizontally. Each variable is listed as a row, making it easier to scan through all columns, even if you’re working with a limited screen space. Here’s how it looks in action:\n&lt; PAUSE &gt;\nNow, here’s a question for you: What happens if you try to use glimpse() in a method chain? Can you chain the operation head() after calling glimpse()? What do you think the output will be?\n\n&lt; PAUSE &gt;\nThe answer is: no, you cannot. glimpse(), does not return you a data frame. Instead, the output of glimpse() is the text printout meant solely for viewing. No further operations can be applied to it. If you attempt to chain additional methods, you’ll encounter an error. Give it a try if you want! Polars will throw an error saying that the head() method cannot be applied to a NoneType, which is the type of output glimpse() returns.\nIf you understand how to use head(), tail(), describe(), and glimpse(), you have powerful tools at your disposal to explore and familiarize yourself with any dataset before diving deeper into your analysis.\n\n\nVIDEO 3 SELECT {greenscreen} ~ 12 min\n\nOne of the most common tasks in data analysis is selecting specific variables or columns from a dataset. Let’s start by pulling out the country information from the clean_fuels data. Pause the video for a moment and try running this code:\n&lt; PAUSE &gt;\nHere, we’re using the select() method to isolate a column. Notice how the column name is wrapped in the pl.col() function. This wrapper explicitly tells Polars that we’re referring to a column in the dataframe.\nBut here’s something cool-you can skip the pl.col() wrapper in certain cases! For example, this code:\n&lt; SMALL PAUSE &gt;\n…does the exact same thing as this:\n&lt; PAUSE &gt;\nPretty neat, right? The select() method can directly interpret strings as column names, making your code a little cleaner and quicker to write.\nWhen you wrap a column name in pl.col(), you’re creating an expression. An expression is like a recepie - it doesn’t do anything on its own. For example, if you run this code:\n&lt; SMALL PAUSE &gt;\n…nothing happens. It just returns something called an “unevaluated expression”. But when you evaluate that expression in the context of a dataset, it returns something useful. For instance:\n&lt; PAUSE &gt;\nHere, the select() method acts as an evaluation environment, turning the pl.col() expression into actual data. select() is one of the several methods in Polars that can evaluate expressions. While select() is highly versatile and can do other things as well, for now, we’ll focus on its simplest use case: extracting columns from a data frame.\n\nThe pl.col() wrapper is super flexible, and it’s going to be central as we build more advanced expressions in Polars. For instance, you can use pl.col() to refer to multiple columns simultaneously:\n&lt; PAUSE &gt;\nSometimes, typing out long column names can feel like a chore, especially when you’re working with many columns. But don’t worry - Polars makes it easy to select columns by their position in the dataset. For example, this code selects the second and third columns by their numerical index:\n&lt; PAUSE &gt;\nNote that the column indices in Polars are 0-based. That means the first column is index 0, the second column is index 1, and so on.\nWhat about negative numbers? They’re a handy shortcut for selecting columns from the back of the dataset. For instance, -1 refers to the last column, and this code will select the first and last columns:\n&lt; PAUSE &gt;\nA note of caution: Selecting columns by the order of their appearance can be risky. If your dataset’s structure changes, you might accidentally select the wrong columns. So, use the nth() function sparingly.\nNow, let’s talk about the opposite of select() - the drop() method. The drop() method removes specific columns from your dataset, leaving everything else intact. For example:\n&lt; PAUSE &gt;\nDropping is equivalent to selecting all columns except the ones you want to exclude. Here’s how could would write it using in terms of selection:\n&lt; PAUSE &gt;\nThe pl.all() function refers to all columns, and the exclude() method lets you refine the selection by removing specific ones.\nA quick reminder: dropping columns doesn’t modify your original dataset. It only affects the result of that query. Unless you explicitly overwrite the original dataframe, everything stays the same. So feel free to experiment!\nNow it’s your turn. Select the columns related to the number and the proportion of the people with access to clean fuels. Try both selecting by name or index, and dropping the columns you dont need.\n\nPlease, pause the video and try couple of different ways of selecting these columns.\n&lt; PAUSE &gt;\nGot it? Great! Both approaches (selecting specific columns or dropping the ones you don’t need) give you the same result. Expressions like these make your analysis more dynamic and efficient, so you can quickly adapt to different datasets or scenarios.\n\n\nVIDEO 4 SELECTORS {greenscreen} ~ 7 min\n\nSelecting columns is such a common task that Polars has a dedicated module for it. It is called polars.selectors. This module provides a collection of methods specifically designed to simplify picking columns from a data frame. polars.selectors is often aliased as cs for convenience.\n\nLet’s make sure we import the selectors module:\n&lt; PAUSE &gt;\nAmong the most useful selectors are, of course, selectors by name and by column index (for which we might not really need selectors, because those can be picked out with pl.col() and pl.nth()).\n&lt; PAUSE &gt;\nSelecting first and last columns are so common, there are useful shorthands cs.first() and cs.last(). To select all columns other than the one you specified, you can use the tilde ~ operator. Tilde operator works with all methods in the cs. module and negates the selection. For example ~cs.last() refers to all columns other than the last one.\nSelectors can target columns based on their data types! For example, cs.numeric() picks all numeric columns. And if you want non-numeric columns, you can just negate it with ~.\nAnd now it is your turn! Practice selecting first, everyhing other than the first, as well as all non-numeric columns. Use selector class for this. Pause the video and give it a try!\n&lt; PAUSE &gt;\n\nFantastic work! With a wide menu of selector methods, plus column and index-based expressions like pl.col() and pl.nth(), Polars gives you incredible flexibility in working with your data. These tools will become invaluable as we move into crafting more complex expressions.\nStay tuned - there’s a lot more to explore!\n\n\nVIDEO 5 FILTER {greenscreen} ~ 12 min\n\nNow let’s talk about filtering - an essential part of data analysis. In Polars, you can use filtering to subset your dataset based on logical conditions, using the magic of expressions. Logical operations are one of the simplest and most common use cases for expressions. For example, you can compare every value in the region column to the string “Europe”. If there’s a match, Polars returns True; otherwise, it returns False.\nLet’s see how this works in code:\n&lt; PAUSE &gt;\nHere, the filter() method applies the logical condition, and only rows where the region is “Europe” are included in the result. Notice that for exact comparisons, we use the double equals sign ==. Similarly, for inequalities, we can use operators like &lt;=, &gt;=, &lt;, or &gt;. Not equal is spelled out as !=.\nBut filtering doesn’t stop there - you can combine multiple conditions to create more complex filters.\nHere’s a challenge for you. Can you find all the countries in Europe where the majority of the population lacked access to clean fuels for cooking at the end of 2022? Take a moment to write this expression. Pause the video if you need to.\n&lt; PAUSE &gt;\n\nWhat did you get? Oh, wow! Over half the population of Bosnia still lacks access to clean fuels for cooking. That’s a striking insight!\nLet’s zoom in on Bosnia to better understand the data. Bosnia’s country code is “BIH”, but you can also use country name to filter, if you prefer.\n&lt; PAUSE &gt;\nWe are interested in tracking how the proportion of the population with access to clean fuels for cooking has changed over the years. Let’s create a plot, with the year on the x-axis and the proportion of population on the y-axis. If you remember from the Plotnine module, the dataset goes into the first argument of the ggplot function.\nHere’s one way to do this:\n&lt; PAUSE &gt;\nThis works, but the code feels a little cluttered. It’s not immediately clear where the dataset comes from.\nLet’s clean this up using the .pipe() method. The .pipe() method hands the data to the ggplot() function, placing it as the first argument.\n\nPipe keeps the code clean and modular. Everything after .pipe(ggplot) is Plotnine-specific code.\n&lt; PAUSE &gt;\nNice!\nWhat if you’re not sure how a country’s name is spelled in the dataset? For example, is it “Czech Republic” or just “Czechia”?\nIn this case, you can use partial string matching to find it.\n&lt; PAUSE &gt;\nHere, we use the str.starts_with() method, which checks if strings in the country column start with the letters “Cz.” Ah, there it is - “Czechia”!\nPolars offers several handy string operations. For example:\n\nstr.starts_with()\nstr.ends_with() and\nstr.contains()\n\nYou’ll see more of these as we progress, but these three are powerful enough to help you tackle the following challenge.\nFilter the data for your own country and visualize the proportion of people with access to clean fuels. Once you’re happy with the subset of your data, use ggplot and everything you’ve learned about Plotnine to create a polished visualization.\n&lt; PAUSE &gt;\n\nThis looks fantastic! Great work visualizing your country’s data.\nIn the next section, we’ll explore adding more columns to our dataset and practice advanced subsetting and visualization techniques. Stay tuned!\n\n\nVIDEO 6 DEATHS DATA {greenscreen} ~ 3 min\n\nLet’s apply what we’ve learned about filtering to visualize the causes of death in some European countries.\n&lt; PAUSE &gt;\nThis dataset contains both summarized and detailed breakdowns of deaths for every country and year. Take a look at the column labeled cause_of_death. When this column says “All causes,” it represents the total deaths for that country and year-a sum of all the other rows.\nLet’s zoom in on Bosnia for a single year, say 2010, to understand this better.\n&lt; PAUSE &gt;\nOne of the rows is labeled “All causes” with 4,816 deaths. This total matches the sum of the individual causes of death. While it’s useful to have the total, it can lead to double counting if we include it in our analysis.\nNow let’s expand our view to include all European countries for which we have death data. We’ll exclude the totals and focus on trends for each specific cause of death. Faceting will help us visualize these trends country by country.\n&lt; PAUSE &gt;\nMost of the trends appear to be decreasing, which is good news. However, even with free y-axis for each country, the differences in scale make it hard to compare trends across countries. Look at Moldova! There’s a dramatic improvement in death cases here. Meanwhile, heart- and stroke-related deaths in the close-by Russia are on the rise.\nIt would be good to put these numbers in perspective using population of these countries. Is this something we could calculate from our clean_fuels dataset?\n\n\nVIDEO 7 WITH_COLUMNS {greenscreen} ~ 11 min\n\nRemember, the clean_fuels data told us both the number of people with access to non-polluting fuels and the proportion of the population they represent, expressed as a percentage. With this, we can reverse-engineer the population for each country.\nTo create new variables, we use the with_columns() method. This method lets us add or modify columns using expressions.\nLet’s start by converting the percentage of people with access to clean fuels into a true proportion by dividing it by 100.\n&lt; PAUSE &gt;\nHere, we create a new column called prop. The expression starts with a column name, and then we specify the operation: dividing by 100. The prop on the left of the equal sign will become a new column name. Easy enough, right?\nNow that we have the true proportion, we can calculate the total population. Let’s divide the population by proportion.\n&lt; PAUSE &gt;\nNotice that I wrapped the division operation in parentheses to ensure it’s evaluated correctly. I also used the alias() method to specify a name for the new column: population. This code calculates and adds two new columns:\n\nprop: the true proportion of the population with access to clean fuels, and\npopulation: the estimated total population for each row.\n\nThis looks good. Lets assign it to a variable and inspect the updated dataset.\n&lt; PAUSE &gt;\nOh, look! We’ve got some NaN, or “Not a Number”, values in the population column. These appear because we divided by zero wherever the proportion was zero. Division by zero is, understandably, illegal in most places - and in Python, it results in NaN.\nNaN is a special marker for missing or undefined values. Missing values in Polars propagate through calculations, which means any further operations on these rows will also result in missing values. This is why the mean and standard deviation of the population column are also NaN in the summary.\nLet’s see how many rows in our dataset contain illegal population estimates. We can use the .is_nan() method, which evaluates whether a column contains NaN values.\n&lt; PAUSE &gt;\nUh-oh! 35 rows! Perhaps fuel_types, the third dataset, could be a better source of population data?\nThe fuel_types dataset contains both the proportion of people using a specific cooking fuel and the absolute number of users. Since it includes multiple estimates for each country and year - one for each fuel type - it might give us more opportunities to calculate valid population estimates.\nGo ahead and add a population column to the fuel_types dataset. Save the extended data frame under a new name - we will need it later.\n&lt; PAUSE &gt;\n\nThis looks promising! Let’s have a look at our favorite Bosnia\n&lt; PAUSE &gt;\nInteresting! We still get some NaN values, for example, look here: in 2022 no one was using Kerosene to cook food. Thanks goodness! But now we get several estimates of Bosnia’s population - 3.0, 3.5, 3.53, 3.48 million. These slight differences arise from rounding imprecisions in the proportions or the number of people using the specific fuel type. While these variations are minor, they make it tricky to work with the data directly.\nWouldn’t it be nice to level out these differences, by say, averaging? In the next section, we’ll learn how to do just that: grouping and aggregating. Stay tuned!\n\n\nVIDEO 8 SUMMARIES {greenscreen} ~ 14 min\n\nOne of the most important tasks for data analysts is creating summaries of the data, particularly by groups. In our newly prepared dataset, we’re interested in summarizing the total population for each country by year. In Polars, this is a two-step process. First, we define the groups using .group_by(). Then, we aggregate the data using .agg().\n&lt; PAUSE &gt;\nThis gives us a mean population estimate for each country and year. But you might notice something peculiar-some countries and years still show NaN values for the population. These missing values are a result of the division by zero we encountered earlier. To address this, we have a couple of options:\nWe could drop the missing values directly before averaging, using .drop_nans(); OR We could filter out zero-valued records in the “proportion of population with access to clean fuels”, which was used in the denominator of the population column.\n&lt; PAUSE &gt;\nNow, let’s build on this idea. What if we wanted to summarize populations at a higher level - say, by region - and visualize how total population changes over time?\nGive it a try! I suggest you start by by calculating the average population for each country and year. Then, you can group this data by region and year, summing the populations for all countries in each region. Finally, plot the results.\nTake your time and think through what type of aggregation you would need to do.\n&lt; PAUSE &gt; \nWhen we visualize the results, the trends are striking. Populations in Africa and Southeast Asia are growing at a much faster pace compared to other regions. For Africa, this growth rate might even be accelerating. On the other hand, regions like Asia and the Western Pacific show signs of population growth slowing down. These insights help us understand global population dynamics and can inform policies in areas like health, infrastructure, and environmental planning.\nNow that we’ve explored population estimates and regional dynamics, let’s take on another challenge: examining the changes in the popularity of different cooking fuels worldwide.\nHere’s your new task: Visualize the total number of people using each type of fuel for cooking, for every year, worldwide.\n&lt; PAUSE &gt; \nThis plot looks fantastic! By aggregating by fuel type and year, we can clearly see trends in fuel usage globally. But we can take this further. What if we wanted to break this data down by region? Faceting would allow us to examine the trends within each world region more closely.\n&lt; PAUSE &gt;\nWith this plot, we can see unique trends for every region. Across the globe, electricity and gas are becoming more prevalent. Yet, biomass remains crucial in regions like Africa and Southeast Asia. However, there’s still a challenge here. Because the number of people using each fuel type varies so widely across regions, it’s difficult to compare the energy mix within each region.\nTo make the energy mix clearer, we can represent the share of the population relying on each fuel type within a region. This requires dividing the number of people using each fuel by the annual total population for the region. We’ll use Polars’ .over() method to calculate these totals for subgroups without collapsing the data.\n&lt; PAUSE &gt;\nI wrap the expression into parenthesis and pass it to the .over() method which defines the scope for the .sum() operation. Note that this is different from aggregation, because the number of rows actually does not change. Lets visualize it!\n&lt; PAUSE &gt;\nWhat a graph! What a story! In Africa, biomass remains dominant, though natural gas is beginning to make inroads. The Americas, Europe, and Eastern Mediterranean rely heavily on gas, with electricity emerging as a significant player in Europe and the Western Pacific. Southeast Asia, meanwhile, is undergoing a rapid transition from biomass to gas - a remarkable shift in just a few decades. This visualization gives us a profound understanding of how cooking fuel usage has evolved globally and regionally over time.\n\n\nVIDEO 9 {greenscreen} ~ 3 min\nIt’s time for you to take on another challenge - one that highlights the global reliance on coal and charcoal for cooking. Create a column chart showing the top 10 countries by the number of people using coal and charcoal for cooking in 2022. Color the columns by continent. How many of these top-10 countries are in Africa?\n&lt; PAUSE &gt; \nSeven - possibly eight, if you include Somalia - of the top-10 countries using coal for cooking are in Africa! Many African nations rely heavily on these highly polluting and dangerous carbon-based fuels. Ensuring access to cleaner, safer energy sources is essential to improving health outcomes and accelerating the transition to sustainable energy.\nIn the next section, we’ll learn how to combine datasets through joins and reshape them using pivots - powerful tools for transforming and enriching our analyses. See you soon, as we dive deeper into the art and science of data manipulation!"
  },
  {
    "objectID": "pivotjoin.html",
    "href": "pivotjoin.html",
    "title": "Pivots and joins",
    "section": "",
    "text": "Hello and welcome back to Data Literacy with Python!\nWe’re continuing our exciting journey into data wrangling—a cornerstone of data analysis and storytelling. If you’ve been with us through the previous modules, congratulations! You’ve covered a lot of ground and gained some serious data skills.\nLet’s quickly recap:\n\nWe explored subsetting data, learning how to select specific rows and columns with functions like .select(), .drop(), and the powerful suite of polars.selectors.\nFor observations, we used .head() and .tail() to view subsets and .filter() to fine-tune our subsetting of the data.\nWe mastered creating new columns using expressions wrapped in .with_columns().\nAnd finally, we learned how to summarize data using .group_by() and .agg(), creating insightful summaries of our datasets.\n\nThese are all foundational skills, and you’re doing great!\nBut now, it’s time to level up. Today, we’re tackling data reshaping and joins, two powerful techniques for reorganizing and enriching your datasets. I also promised you some nice-looking tables, and I intend to deliver! We’ll be using the amazing great_tables library for our table designs. Let’s load up the libraries and dive right in.\n\n\nCode\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import GT\n\n\nToday’s dataset comes from the Break from Plastics environmental campaign — a sample of data with a powerful story. Here’s the description of the data:\n\n\n\n\n\n\nNote\n\n\n\n\n\nIn 2020, thanks to our members and allies, Break Free From Plastic engaged 14,734 volunteers in 55 countries to conduct 575 brand audits. These volunteers collected 346,494 pieces of plastic waste, 63% of which was marked with a clear consumer brand. Despite the challenges of organizing during a global pandemic, our volunteers safely coordinated more brand audit events in more countries this year than in the previous two years. As a special activity during the pandemic, we also worked with over 300 waste pickers to highlight their roles as essential workers. Participants catalogued over 5,000 brands in this year’s global audit. Our analysis reveals the following as the 2020 Top 10 Global Polluters: The Coca-Cola Company; PepsiCo; Nestlé; Unilever; Mondelez International; Mars, Inc.; Procter & Gamble; Philip Morris International; Colgate-Palmolive; and Perfetti Van Melle.\n\n\n\nHere’s the code to bring this data into our workspace:\n\n\nCode\nplastics_df = pl.read_csv('bffp/BFFplastics.csv')\n\nplastics_docs = pl.DataFrame({\n    'Variable': ['region', 'country_code' , 'country', 'year', 'parent_company', 'empty', 'hdpe', 'ldpe', 'o', 'pet', 'pp', 'ps', 'pvc', 'grand_total', 'num_events', 'volunteers'],\n    'Class': ['character','character','character', 'double', 'character', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double', 'double'],\n    'Description': ['Region', 'Alpha 3 ISO 3166 code','Country of cleanup', 'Year (2019 or 2020)', 'Source of plastic (company name)', 'Category left empty count', 'High density polyethylene count (Plastic milk containers, plastic bags, bottle caps, trash cans, oil cans, plastic lumber, toolboxes, supplement containers)', 'Low density polyethylene count (Plastic bags, Ziploc bags, buckets, squeeze bottles, plastic tubes, chopping boards)', 'Category marked other count', 'Polyester plastic count (Polyester fibers, soft drink bottles, food containers (also see plastic bottles)', 'Polypropylene count (Flower pots, bumpers, car interior trim, industrial fibers, carry-out beverage cups, microwavable food containers, DVD keep cases)', 'Polystyrene count (Toys, video cassettes, ashtrays, trunks, beverage/food coolers, beer cups, wine and champagne cups, carry-out food containers, Styrofoam)', 'PVC plastic count (Window frames, bottles for chemicals, flooring, plumbing pipes)', 'Grand total count (all types of plastic)', 'Number of counting events', 'Number of volunteers']\n})\n\n\nIn your notebook you can see the code for importing the data, as well as a DataFrame containing a data dictionary — a detailed description of the variables in this dataset.\nSo far, we’ve always imported data from CSV files or pre-built datasets. But now, it’s time to talk about creating data frames by hand. This is super handy when working with small examples, prototypes, or mock data. To do that, we need to talk about two foundational Python data structures: dictionaries and lists.\nThink of a dictionary as a way to describe an object. It’s a collection of “key-value” pairs—like writing down standard characteristics of something along with their values. Dictionaries are specified with curly brackets {}. Let’s say I want to describe my bike:\n\n\nCode\nBicycle = {\n    'Type': 'Hybrid',\n    'Size': 28,\n    'Make': 'Merida',\n    'Color': 'Grey',\n    'Price': 250\n}\n\n\nHere, each characteristic—like Type or Size—has one value. Easy, right? But what if I also wanted to describe the bikes of my twins? I’d need three records, not one.\nThis is where lists come in. A list is a collection of items—typically of the same type—and it’s denoted with square brackets []. Let’s use lists to describe all the bikes in my garage:\n\n\nCode\nBikes = {\n    'Type': ['Hybrid', 'MTX', 'BMX'],\n    'Size': [28, 24, 26], \n    'Make': ['Merida', 'Giant', 'Specialized'],\n    'Color': ['Grey', 'White', 'Orange'],\n    'Price': [250, 180, 220]\n}\n\n\nNow, we have a dictionary of lists, representing three bikes. To turn this into a Polars DataFrame, all we need to do is pass it to the pl.DataFrame() function:\n\n\nCode\nbikes_df = pl.DataFrame(Bikes)\n\n\nAnd just like that, we’ve created a data frame by hand! This is a simple yet powerful way to structure and manipulate small datasets.\nOur plastics_docs specifies three characteristics: the variable names (Variable), their data types (Class), and their descriptions (Description).\nThis table essentially acts as documentation for the plastics_df. But let’s face it — raw data frames, while functional, don’t always look polished or presentation-ready. That’s where the Great Tables package comes in.\nGreat Tables is like a graphic design toolkit for your tables! It introduces a “grammar of tables,” similar to how plotnine provides a “grammar of graphics.” This makes it super easy to transform plain data frames into beautifully styled tables with minimal effort.\nThe core function in Great Tables is GT(), and it works similarly to how we use ggplot for creating plots. Let’s take a sneak peek at its capabilities by styling our plastics_docs data frame. Here’s how we do it:\nVoilà! .opt_stylize() method has some pre-built styles, which we used to convert a boring data frame into a polished and professional-looking table, ready to be shared or included in reports. Don’t worry about memorizing the details just yet — we’ll explore GT() more thoroughly in the upcoming sections.\nBefore we move on, let’s take a quick look at the data itself. Here’s a snippet of the first five rows of the plastics_df:\nAs you can see, the first few variables look familiar. They describe general metadata, like the region, country, and year. But let’s focus on the variables starting from empty and going down to pvc. These columns count the number of plastic pieces of different types collected during the cleanup.\nThe grand_total column sums up all these individual plastic counts. Finally, the last two columns—num_events and volunteers—capture operational details:\n\nHow many trash counting events took place in each country during a given year?\nHow many volunteers participated in these campaigns?\n\nThis dataset offers a wealth of insights into plastic pollution patterns across the globe. By organizing, reshaping, and visualizing this data, we’ll uncover powerful stories about the environmental challenges we face—and the steps we can take to address them.\nLet’s take a closer look at the data. Check out this very first row for Argentina. Notice the parent_company column contains the value: “Grand Total.” This suggests that this first row contains the totals for all Argentinian records in 2019.\nLet’s look at the next year:\nHere’s something curious: for the year 2020, the rows with country totals aren’t marked with “Grand Total.” Instead, the parent_company field is left blank, or in technical terms, it’s marked as missing - Null. Hmm! What do we do with those?\nNull values aren’t just limited to parent_company. Let’s take a look at records collected in 2019 from unidentified locations.\nBefore we dive deeper into analyzing top contributors to plastic waste, let’s calculate the totals per country and year ourselves. Why?\nWell, the dataset has pre-computed totals marked with “Grand Total” or Null in the parent_company, but the logic seems inconsistent. Recomputing the totals ensures transparency and accuracy in our analysis.\nHere’s how we start:\nLet’s break this down:\n\ndrop(\"grand_total\"): The grand_total column is a pre-computed sum of all plastic types, which we can recalculate if needed.\nExclude “Grand Total” rows: We filter out rows where the parent_company column is populated with the phrase “Grand Total.”\nExclude Null values in parent_company column: These rows lack a meaningful company label and often represent aggregated data.\n\nBy cleaning the data in this way, we ensure that our analysis is based on individual contributions, not pre-summarized totals."
  },
  {
    "objectID": "pivotjoin.html#stacking",
    "href": "pivotjoin.html#stacking",
    "title": "Pivots and joins",
    "section": "Stacking",
    "text": "Stacking\n\n# indoor air pollution\nhhap_deaths = pl.read_csv(\"hhap/hhap_deaths.csv\")\nclean_fuels = pl.read_csv(\"hhap/clean_fuels_cooking.csv\")\nfuel_types = pl.read_csv(\"hhap/cooking_by_fuel_type.csv\")\n\nAnother common type of operation is stacking two identical datasets together (vertically). This is possible to do when the meaning of the columns in the datasets is the same and we are interested in combining two parts of identical data into a new and larger dataset.\nRecall that in our household air pollution case study we had three files: - hhap_deaths - containing death cases, associated with air pollution - fuel_types - describing information about the fuels used by population in different countries for household needs - clean_fuels - containing the fraction of population in each country with access to clean fulels for cooking\nAll three of these datasets contain three identical columns describing the country of observation: region, country_code and country. The countries listed in each of the datasets is largely similar, but not completely overlapping. Let’s see if we can compile a single master set of all countries with the codes and the regions they belong to. Because the data is recorded over many years each of the datasets contains many duplicates entries. This problem will be even larger when we stack the data from several datasets together, so we will need to ensure the records in our final (combined) dataset are unique.\n\nidcols=cs.by_name(\"region\", \"country_code\", \"country\")\ncountry_regions = (hhap_deaths.select(idcols)\n    .vstack(fuel_types.select(idcols))\n    .vstack(clean_fuels.select(idcols))\n    .unique()\n)\n\nNote, that here we created a temporary object idcols, which will store only selector object for the three columns we are interested in. Polar selectors are independent entities which can live both inside the querying contexts as well as in the global environment, i.e. in memory accessible\nLets compare our country codes with the full list of codes issued by ISO. Here’s a file with all Alpha 2 and Alpha 3 codes issued to nation states and territories.\n\niso_df = pl.read_csv(\"hhap/CountryCodes_Alpha2_Alpha3.csv\")\n\n(country_regions\n    .join(iso_df, left_on=\"country_code\", right_on=\"alpha3\", how=\"anti\"))\n\n(country_regions\n    .join(iso_df, left_on=\"country_code\", right_on=\"alpha3\", how=\"left\"))\n\n# How many countries are not present in the combined household air pollution dataset? \n# What proportion of those countries have the world \"Island\" in their name?\n\ntmp_df1 = (iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\"))\n\n(iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\")\n    .select(pl.col(\"country\").str.contains(\"Island\").mean())\n    )\n\n(iso_df\n    .join(country_regions, left_on=\"alpha3\", right_on=\"country_code\", how=\"anti\")\n    .group_by(pl.col(\"country\").str.contains(\"Island\").alias(\"island\"))\n    .len()\n    .with_columns(pl.col(\"len\")/pl.sum(\"len\"))\n    .filter(\"island\")\n    )\n\n\nshape: (1, 2)\n\n\n\nisland\nlen\n\n\nbool\nf64\n\n\n\n\ntrue\n0.259259\n\n\n\n\n\n\nHorizontal stacking is possible, but you probably want to do a join instead, because horizontal stacking assumes that row order is the same and observations are identical. This is better ensured with unique IDs which could be used for join."
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "VIDEO 1 {greenscreen} ~ 7 min\n\nWelcome to the lesson on data visualization!\nData visualization is one of the most important skills in data analysis. Why? Because a well-made chart can reveal patterns, trends, and insights that might otherwise stay hidden in a spreadsheet. It’s like turning a jumble of numbers into a picture that tells a story.\nBut let’s be honest—visualizing data can sometimes feel overwhelming. There are so many types of charts to choose from and endless options for customizing them. Scatter plots, bar graphs, heatmaps—what should you use? And even after you pick a plot, there are all these parameters: axis, labels, color scales, gridlines… It’s easy to feel like you’re drowning in options!\nPoint-and-click tools for visualization, like those built into some software, can be helpful, but they come with their own challenges. They often overwhelm you with choices, and worse, they don’t always give you an easy way to reproduce or share your work.\nWhen you create visualizations using a script instead of a mouse, you unlock an entirely new level of power. Scripting your plots means they’re reproducible. You can tweak them, reuse them, and share the code with others. It’s like building a recipe that others can follow, modify, or inspect to understand exactly how the visualization was made.\nNow, let’s take a step back into history for a moment. In 1999, a statistician named Leland Wilkinson published a groundbreaking book called The Grammar of Graphics. Think of it like this: Just as grammar gives structure to language, Wilkinson’s framework gave structure to statistical graphics. He introduced a way to think about and construct plots systematically, rather than relying on intuition or tradition alone.\nHis ideas were revolutionary and influenced countless tools for making visualizations. One of the most famous examples is the R package ggplot2, created by Hadley Wickham. Wickham built on Wilkinson’s Grammar of Graphics and created what is now considered one of the most powerful and popular visualization tools in the world of data science.\nIn 2017 a passionate Python developer from Uganda by the name Hassan Kibridge ported ggplot2 into Python. His project became known as plotnine and it soon became a universal success. Here’s the story of plotnine in his own words:\n\n\nWhile the syntax of plotnine might feel a bit different from typical Python code, don’t worry—there’s a reason for it! The goal is to keep the grammar intact, and that consistency makes it easy to learn and incredibly flexible to use.\nIn this lesson, we’ll dive into plotnine and explore how it allows you to create clear, beautiful, and insightful visualizations. We’ll guide you step by step so you can quickly become comfortable with its intuitive and expressive syntax.\nSo, join us on this journey into the wonderful world of data visualization. By the end, you’ll be creating plots that don’t just look good but also communicate your data’s story effectively. Let’s get started!\n\nBefore we dive in, let’s talk about the tools and libraries we’ll be using in this lesson.\nFirst up is polars, the powerful data analysis library we’ll be relying on throughout the course. If you’re familiar with Python, you know it’s common practice to use shorthand or aliases when importing libraries. For polars, the standard alias is pl, so that’s what we’ll use here. Anytime we call a function from polars, it will be prefixed with pl. — simple and consistent.\nNow, when it comes to our visualization library, plotnine, we’ll take a slightly different approach. Instead of using a prefix, we’ll import all its functions directly into our workspace. This means we’ll use the from plotnine import * syntax, which essentially says, “Hey Python, bring in everything from plotnine!” Why? Because it makes our plotting code cleaner, easier to read, and more expressive.\nFinally, let’s talk about the dataset we’ll be exploring today. It comes from the gapminder package. If you’re not familiar with Gapminder, it’s a non-profit organization founded by Hans Rosling and his children back in 2005. Their mission? To promote a better understanding of global development through data—focusing on health, economics, and the environment.\nThe Gapminder Foundation maintains an incredible collection of statistics about the world, and this package is a small extract from their database. It’s packed with fascinating data on public health, economic development, and global welfare.\nHans Rosling himself is famous for his captivating TED Talk in 2007, where he used data to tell the story of global development. He spoke about life expectancy, GDP, and even the humble washing machine—and how it changed the world. If you haven’t watched that talk yet, I can’t recommend it enough. It’s a masterclass in how to make data come alive.\nSo, with our tools in hand and an inspiring dataset at our fingertips, we’re ready to start exploring and visualizing. Let’s get to it!\n\n\nVIDEO 2 {greenscreen} ~ 5 min\n\nThe dataset has been conveniently imported for us by the gapminder package. To get started, we can simply type gapminder into our console and hit Enter. When we do that, we’ll see a preview of the data in the form of a table — what we call a DataFrame. In this table, each row represents an observation, and each column represents a variable.\nThis dataset has 1,704 rows and 6 columns, so it’s fairly compact but still rich with information. Let’s walk through what each of these columns means:\n\ncountry: This column lists the names of countries. If you take a look at the data, you’ll notice it starts with Afghanistan at the top and ends with Zimbabwe at the bottom. It seems the data is sorted alphabetically by country.\ncontinent: Here, we have the names of continents. For example, Afghanistan is listed under Asia, while Zimbabwe is under Africa. Makes perfect sense.\nyear: This column tells us the year of the observation. You’ll notice that each country has multiple rows because data was collected at different times. The dataset starts in 1952 and progresses in 5-year increments, which gives us a nice snapshot of changes over time.\nlifeExp: This column stands for life expectancy at birth. If we look at Afghanistan in 1952, for example, the life expectancy was just 28.8 years. Let that sink in for a moment — only 28 years! It’s a sobering reminder of the challenges some nations faced in the mid-20th century.\npop: This column shows the population of each country. Again, looking at Afghanistan in 1952, the population was just under 8.5 million people.\ngdpPercap: Finally, this column contains the GDP per capita, expressed in US dollars. From what I understand, these figures have been adjusted for inflation, so they should be comparable across countries and over time.\n\nAltogether, these six columns give us a fascinating lens through which to explore global trends in health, wealth, and population growth. The dataset might look simple at first glance, but it’s packed with stories waiting to be uncovered.\nNow that we know what we’re working with, let’s roll up our sleeves and start exploring!\n\n\nVIDEO 3 {greenscreen} ~ 10 min\n\nNow, it’s time to create our very first plot! Here is a question we would like to answer using gapminder data: Do people in rich countries live longer than people in poor countries?\n\nThe answer may be quite intuitive, but we will continue our investigation further: How does the relationship between GDP per capita and Life expectancy look like? Is this relationship linear? Non-linear? Are there any exceptions to the general rule?\n\nIn order to answer these questions, we will create a plot from gapminder data. Here’s the code we’ll use. Take a moment to copy this code verbatim from your screen.\nWhen writing Python code with plotnine — and later when we use polars — you’ll notice that we often wrap our code in parentheses. This is a great habit to get into because it allows us to break our code into multiple lines without worrying about indentation.\nLet’s walk through this step by step.\nInside the outer parentheses, the first thing we write is ggplot. This is the foundational function in plotnine, and it stands for Grammar of Graphics plot. Then, in parentheses again, we pass the dataset we want to use — in this case, gapminder.\nAfter that, we add a plus sign. The Grammar of Graphics, which plotnine is built on, thinks of plots as being made up of layers. The + sign we added tells Python that we’re adding more layers or components to our plot. Think of it as saying, “Wait, there’s more!”\nOn the next line, we write geom_point. This is the function that specifies the type of layer we’re adding to our plot. In this case, it’s a point plot, which means we’ll be drawing points on a graph. Without this layer, our plot would just be an empty canvas.\nInside the geom_point parentheses, we specify the argument: mapping followed by an = sign. This tells plotnine how we want to relate our data to the graph. AES stands for “aesthetics”. The inside of the aes function defines mapping of the variables in our data to certain aesthetical properies of our graph. We’re saying, “Take the GDP per capita (gdpPercap) and map it to the x-axis, and take life expectancy (lifeExp) and map it to the y-axis.” Notice that the column names are enclosed in quotes — that’s important!\nOnce you’ve written the code, go ahead and hit the Run button. If everything is correct, you should see your plot appear on the screen.\nLet’s take a moment to reflect on what we just did.\nIn our code, the first layer was the ggplot function, where we provided the dataset. The second layer was geom_point, which added points to our graph.\nThe result is a simple yet meaningful scatter plot. It shows a positive, non-linear relationship between GDP per capita on the x-axis and life expectancy on the y-axis. Does this align with what you initially expected? Or does it challenge your assumptions? Already, you can see how visualizing data helps uncover patterns and stories that might not be obvious at first glance.\nTake your time to review the code and compare it to the plot we created. Understanding this connection — how the code you write translates directly into what you see on the screen — is the key to mastering data visualization.\nIn fact, the structure of most plots in plotnine (and its R counterpart, ggplot2) can be summarized with a simple template:\n\nThis template is incredibly flexible and serves as the foundation for almost every visualization we’ll create.\nIn the remainder of this lesson, we’ll explore how to extend and customize this template to create a wide variety of visualizations. Each new element we add will open up even more possibilities.\nI’ll see you in the next one!\n\n\nVIDEO 4 {greenscreen} ~ 9 min\n\nHello again! Ready for a challenge? I’ve got a question for you: How has life expectancy changed over time? \nTake a moment to think about it. Better yet, try answering it by modifying the code we wrote in the last lesson.\nHere’s a quick hint before you pause the video: The gapminder dataset includes a column called year, which can go on the x-axis. Use that to tweak the code and see what you find. I’ll wait right here while you try it out!\nPause the video now and give it a shot. See you in a moment!\n\nDone? Excellent! Let’s take a look at what we’ve got. Nice work! You should see a scatter plot showing life expectancy over time.\nHmm… notice how some of the points are stacked on top of each other? That’s called overplotting, and it’s pretty common when you have a lot of data points at the same x or y values. Don’t worry—it’s easy to fix!\nInstead of geom_point, try using geom_jitter. This will add a tiny bit of random noise to spread out the points so they’re easier to see.\nHere’s how you do it:\nRun this code and check out the difference. Much better, right? Now we can see the points more clearly.\nLet’s keep going with this little game. Here’s your next challenge: Can you visualize life expectancy by continent?\n\nThink about which variable should go on the x-axis this time. Which continent do you think tends to have the highest life expectancy? Modify your code and give it a shot. Pause the video, try it out, and come back when you’re ready.\n\nGreat job! What do we see here? Looks like life expectancy in Oceania is quite high, although there aren’t many points for that region. Europe is a close second. On the other hand, Africa seems to have the lowest life expectancy overall, judging by the density of points at the lower end of the y-axis.\nHere’s another question: Which continent has the widest spread in life expectancy values? That’s right—it’s Asia. There’s quite a bit of variation there, which is something we’ll dig into in more detail later in the course.\nFantastic work so far! Take a moment to review what you’ve done, and I’ll see you in the next section!\n\n\nVIDEO 5 {greenscreen} ~ 7 min\n\n&lt;…Walks in, looking thoughtful….&gt;\nOh, hey there! You know, I’ve been thinking—what if we could combine the graphs from the last two challenges and show the relationship between not just two variables, but three?\nNow, don’t worry—we’re not diving into “three-dimensional” plots just yet. Instead, we can represent a third variable using color. Let me show you what I mean.\nHere’s modified code that maps the continent variable to the color aesthetic:\nRun this code and take a look.\n\nWhat do you see? Now we can see more clearly how life expectancy has changed over time by continent. For example, the points representing Africa stay clustered near the lower end of the y-axis throughout the years, while Europe’s points are generally higher. Oceania is there too, but it’s barely noticeable because there are so few observations. Pretty cool, right?\nNow, I’ve got a question for you: What happens if we switch the mappings of continent and year? Give it a try!\n\n\nDone? Great! Do you still find this graph useful? Why or why not?\nNow let’s tweak it a bit more. What if, instead of mapping color to year, we mapped it to country? Give it a try!\n\nWhat changed? How does mapping color to country differ from mapping it to year? Take a moment to think about it. What do you think is the main limitation of using the color aesthetic?\nAlright, here’s one last challenge for this section: Can you add a splash of color to our original graph of life expectancy by GDP per capita? Let’s color the points by continent.\n\nTake a moment to run your code and see what you get.\n\nAmazing! By adding color, we can now spot trends and patterns more easily. But did you notice something else? There are a few outliers in this plot. Can you tell which continent those points belong to?\nThe points look a little crowded. But you know, you can always transform GDP per capita to a logarithmic scale for better visualization. Just add scale_x_log10() as an additional layer to your graph, like that:\nYou’re making fantastic progress! Well done! In the next section, we’ll explore even more aesthetics that can help us tell richer stories with our data. See you there!\n\n\nVIDEO 6 {greenscreen} ~ 10 min\n\nHello again!\nSo far, we’ve explored some powerful ways to visualize data using the x, y, and color aesthetics. With these, we’ve been able to represent three variables in a single plot. Pretty amazing, right?\nNow, let’s quickly recap what we’ve learned about the color aesthetic. When we map a categorical variable like continent to color, plotnine automatically picks a distinct palette for each category. This works great when there are just a few categories, but as the number of categories grows, the colors start to blur together and lose their effectiveness.\nOn the other hand, when we map a continuous variable like year to color, we get a gradient. While individual values can be harder to pinpoint, the overall trends are beautifully highlighted by the gradient’s brightness.\nAlright, as promised, let me introduce you to another fantastic aesthetic: size.\nImagine we could vary the size of the points in our graph to represent something meaningful—like the population of a country. That would let us visualize not three, but four variables at the same time. Let’s give it a shot. Here’s the code:\nWRun this and take a moment to appreciate the result. Isn’t it gorgeous?\nNow we can see the journey of countries like China and India over time. Their points stand out because of their large populations. Under the logarithmic transformation of the x-axis, the relationship between GDP per capita and life expectancy starts to look more linear—but not quite!\nNotice the outliers on the far right? They all seem to be from Asian countries. Are these countries rich or poor? Rich, right? But their life expectancy doesn’t quite follow the trend we see in Europe or the Americas. Fascinating, isn’t it?\nNow, let me share one more aesthetic property with you: shape.\nShape can be a great tool for visualizing low-cardinality categorical variables, like continent. Instead of just using circles, we can use distinct shapes for each category. This lets us pack even more information into the same graph.\nReady for a challenge? Let’s push the limits and visualize five dimensions in a single plot. Modify the previous example to map year to color and continent to shape. Take a moment and try it. I’ll wait.\n\n\nWhat do you notice? Can you tell whether those Asian outliers come from small or large countries? Are they from earlier or later time periods?\nThese are the kinds of questions we can answer when we use multiple aesthetics thoughtfully. Isn’t it amazing how much information we can pack into a single visualization?\nFantastic work today! In the next lesson, we’ll continue exploring new tools and techniques to take your visualizations even further. See you soon!\n\n\nVIDEO 7 {greenscreen} ~ 7 min\n\nWelcome back!\nSo far, we’ve packed a lot of information into single graphs using data-mapped aesthetics like color, size, and shape. While this approach is powerful, let’s face it—combining too many aesthetics can make a plot feel busy and overwhelming.\nSometimes, less is more. A clean and simple graph, highlighting just one or two aspects of the data, can be just as insightful—and a lot easier on the eyes.\nNow, the default style in plotnine is already quite nice, but there may come a time when you want to tweak things to better suit your storytelling. So, let’s look at how to customize graphs using non-data-linked properties—those that aren’t mapped to a variable but instead apply globally to all points in the graph.\nHere’s an example. What if we want all the points in our plot to be the same color, say blue? And what if we also want to adjust their size and transparency? Here’s the code to do that:\nGo ahead, give this a try.\n\nBeautiful, isn’t it? All the points are now blue, with a larger size and a soft transparency that makes overlapping points blend together nicely. This transparency, or alpha, helps highlight areas where the data is dense—like shadows on a heatmap.\nNotice something? The color, size, and alpha settings aren’t part of the aes() function. That’s because these properties aren’t mapped to any variable in the data. Instead, they’re applied uniformly to every point in the plot.\nLet’s break it down:\n\ncolor=\"blue\": The color is set as a character string, wrapped in quotes. You can experiment with other colors too—try red, green, or even hex codes like “#FF5755”.\nsize=2: The size of the points is specified as a number, in millimeters. Increase the size to make the points larger or decrease it for smaller ones.\nalpha=0.1: Transparency is a decimal value between 0 and 1, where 0 is completely transparent and 1 is fully opaque.\n\nFinally, let’s talk about shapes. In plotnine, shapes are represented by numbers. For example:\n\n0 is a square,\n1 is a circle,\n2 is a triangle,\n20 is a small filled circle.\n\nHere’s a challenge for you. Change the shape argument in the code to explore different shapes. Try values between 0 and 25, and see how your graph changes. You’ll find the full list of shapes in the plotnine documentation. \nSo, what do you think? With just a few tweaks, we’ve turned our scatter plot into a clean and stylish visual. Customizing non-data-linked properties like this is a great way to emphasize certain elements of your data without overwhelming your audience.\nIn the next lesson, we’ll explore even more ways to take your visualizations to the next level. See you there!\n\n\nVIDEO 8 {greenscreen} ~ 12 min\n\nWelcome back! Let’s dive into another exciting aspect of creating visualizations in plotnine: geometrical objects, or geom_ functions.\nThese geom_ functions are the building blocks of your plots, allowing you to highlight different aspects of your data. By swapping or combining geom_ layers, you can tell entirely new stories with the same dataset.\nFor example, what if we wanted to show the development of life expectancy over time for each country? We could use geom_line() to connect individual data points belonging to the same country.\nHere’s the code to do just that:\nNote that we have a new aesthetics called group. It indicates which points need to be connected together to for a line. Here we are drawing one line per country. Take a moment to run this and see what you get.\n\nDo you see it? Each country now has its own line, colored by continent. It’s fascinating to watch life expectancy trends unfold over time. But look closely—you might notice some sharp, sudden drops for certain countries. What do you think caused these declines? Wars? Epidemics?\nWe’ll learn how to zoom in on these tragic moments and identify the affected countries later in the course, once we’ve mastered some data wrangling with Polars. For now, make a mental note of this question so you can return to it later.\nAnother powerful geometrical object is geom_boxplot(). This creates a “box-and-whisker” plot that illustrates the distribution of values within categories.\nFor example, let’s visualize how life expectancy varies by continent:\nRun the code and take a look.\n\nThe box represents the interquartile range—the middle 50% of data—while the line inside the box marks the median. The “whiskers” extend to show the 95% confidence interval, and any points outside this range are plotted as individual outliers.\nNow, wouldn’t it be great to combine this boxplot with our jittered points from earlier? This would help us see both the overall distribution and the outliers more clearly. Let’s layer them together:\n\nLooks great, doesn’t it? But notice something—there’s some duplication in our code. We had to repeat the same mappings for both geom_jitter and geom_boxplot. That’s fine for now, but it can become cumbersome as your visualizations grow more complex.\nHere’s a trick to make your code cleaner: you can move shared mappings to the parent ggplot() function. This way, every layer will “inherit” these mappings automatically:\nSee? No more repeating yourself! You can still add layer-specific settings or arguments within individual geom_ functions if needed.\n\n\n\n\n\n\nTip\n\n\n\nWhen building complex plots, start by adding one layer at a time. Once you’ve got the basic structure, move any common arguments up to the ggplot() function. This keeps your code tidy and easier to read.\n\n\nGreat job so far! In the next lesson, we’ll explore even more ways to enhance your visualizations. See you there!\n\n\nVIDEO 9 {greenscreen} ~ 12 min\n\nWelcome back! Now, let’s take a closer look at the relationship between GDP per capita and life expectancy.\nAt first glance, life expectancy seems to improve as countries get richer. But is this relationship consistent across continents? Let’s find out by adding trend lines to our plot.\nTrends are essentially linear regression lines. You might remember them from school—they represent the best-fit line through your data. Here’s how we can add them to highlight differences in this relationship by continent:\nTake a moment to run the code and see the result.\n\nWhat do you observe? By default, geom_smooth() creates a regression line for each continent, and plotnine even adds confidence intervals—those shaded gray areas around the lines. These intervals give us an idea of how well the model fits the data.\nWe also used the alpha argument to make our points semi-transparent. Why? It reduces visual clutter and lets the trend lines stand out more. Did you know that transparency can also be mapped to a variable? That’s right—just like color or size, you can use alpha as a mapping aesthetic to make transparency vary based on your data. Try experimenting with that later!\nHere’s a task for you: Modify the code we just used so that instead of creating separate regression lines for each continent, plotnine creates a single trend line for all data points.\nHere’s the code to start with:\n\nTake a moment to think about it. How can you combine the points colored by continent with a single global regression line?\nThere’s more than one way to solve this problem—see what you can come up with!\nDid you find this challenge hard? It’s ok! Let’s step through it together!\nIn our previous example, we declared all the mappings—x, y, and color—at the global level, in the ggplot() function. This means that every layer inherited these mappings. While this works well for most situations, it’s not what we need here.\nTo build a single trend line for all data points, we must ensure that the color aesthetic applies only to the points and not to the trend line. How do we do that? By moving the color mapping from the global level into the geom_point() function.\nHere’s how the updated code looks:\nBy moving the color aesthetic into geom_point(), it now affects only the points layer. Notice that it’s still wrapped in the aes() function because it remains a data-linked property. Meanwhile, the trend line—added by geom_smooth()—inherits only the global mappings for x and y. This creates a single linear model across all continents, as we wanted.\nTake a moment to observe how this subtle adjustment changes the visualization and makes the trend line easier to interpret.\nSome of you might have come up with an alternative solution. Instead of changing the color aesthetic’s scope, we can override it directly within the geom_smooth() layer. In this case, the color aesthetic remains global, but we specify a non-data-linked property for the trend line, such as making it black:\nHere, geom_smooth() ignores the global color aesthetic and instead applies the color black uniformly to the trend line. The result? A single black trend line stands out clearly, while the points remain color-coded by continent.\nBoth approaches work well, and the choice depends on how you want to structure your code and highlight different layers. Managing global and layer-specific mappings is a powerful feature in plotnine that gives you flexibility in creating clean, insightful plots.\n\n\nVIDEO 10 {greenscreen} ~ 5 min\n\nDo you want to learn a nifty trick that can improve your data visualization? This method can be useful when you want to visualize a continuous variable which has a limited number of distinct values.\nImagine we’re working with year, which is technically a continuous variable. But for some visualizations it might make more sense to treat each year as a separate category. How do we do that without modifying the data?\nSimple! Instead of referencing year as a string ('year'), wrap it in factor(), like this 'factor(year)'. This shorthand plotnine function converts a continuous variable into a categorical one on the fly, with each distinct value treated as its own category.\nLet’s put this into practice with a couple of challenges!\nCreate a boxplot of life expectancy over time, treating year as a categorical variable. Using this plot, can you detect when the interquartile range of life expectancy—the middle 50% of values—was the smallest?\nThen apply the same concept to gdpPercap.\nCreate a boxplot of GDP per capita by year, but this time keep it on a logarithmic scale. Remember that we used the scale_y_log10() function to make the data easier to interpret. Compare the interquartile range of GDP per capita in 2007 with that in 1952. Is the world today more or less diverse in terms of economic inequality?\n\nGo ahead and give it a try. Pause the video and come back once you have your answer!\nLooking at these two plots, you might notice a fascinating pattern.\nYou’re absolutely right: economic inequality has grown in the recent decades. In 1952, the world was much poorer, but there was a greater sense of uniformity across nations. By 2007, while the world is significantly wealthier on average, the disparities have widened.\nAnd those outliers? Intriguing, aren’t they? Three countries stand apart from the rest in the 1952. Which ones could they be? We’ll revisit these mysteries after diving into data wrangling techniques.\nGreat work on these challenges! These exercises show the power of visualizing data in different ways and how little tricks like factor() can make your plots much clearer. Next, we’ll explore other types of plots that can uncover even more insights. Stay tuned, and I’ll see you in the next lesson!\n\n\nVIDEO 11 {greenscreen} ~ 12 min\n\nBy now, you’ve learned so much about using plotnine to create insightful visualizations. But we’ve barely scratched the surface!\nOne of the most exciting features of plotnine is the sheer variety of geoms—the building blocks for visualizing data. Start typing geom_ in your code editor, and you’ll see a list of options pop up. It’s like a treasure chest of possibilities, and each geom offers a unique perspective on your data.\nLet’s put your skills to the test with a few new challenges!\nHistograms are perfect for exploring the distribution of a single variable. Let’s start with life expectancy. Create a histogram and observe the shape of the distribution. How many peaks—or modes—does it have? Play around with the bins parameter. Adjusting the number of bins changes the granularity of your histogram, which can affect how you interpret the distribution. What value of bins seems reasonable to you?\n\nNext up: density plots. These are smoothed-out versions of histograms, showing the probability distribution of your data.\nCreate a simple density plot for life expectancy. You can do it! Start typing and you will find the function you need. Do you see it? What if you want to compare distributions across continents? Add a color aesthetic.\n\nRight! You can split the data by continent by adding a color aesthetic and linking it to the variable continent. Or take it one step further! Use the fill aesthetic (in addition to color) to fill the areas under the curves. Add some transparency with alpha for a cleaner visualization like this:\nThese plots help us see how life expectancy varies not just overall, but also within each continent. Notice any interesting patterns? What might explain the peaks—or modes—you see in the distributions?\nNow let’s level up with 2D density plots. These are excellent for visualizing relationships between two variables. Start by creating a density plot of log GDP per capita vs. life expectancy (use geom_density_2d() function):\n\nWhat do you see? Notice the two distinct clusters? One cluster represents countries that are poorer and have lower life expectancy, while the other includes those that are wealthier and healthier.\nNow let’s break it down by continent. Add a color aesthetic to see how regions of the world are distributed.\n\nIsn’t that fascinating? The lower cluster is primarily made up of African countries, while the higher cluster mostly includes Europe and Oceania. Asia? It’s scattered across both clusters, reflecting its diversity in economic and health outcomes. These exercises highlight the flexibility and power of plotnine. Whether it’s histograms, density plots, or advanced 2D density visualizations, each plot adds a new layer of understanding to your data.\n\n\nVIDEO 12 {greenscreen} ~ 10 min\n\nWhen your graph starts to feel a bit too crowded—perhaps with too many layers or overlapping aesthetics—there’s a simple solution: faceting. Faceting allows you to split your data into separate panels, creating multiple similar graphs for subsets of your data. This can make complex trends easier to spot and comparisons much clearer.\nIn plotnine, faceting is incredibly easy to use. Let’s revisit one of our earlier graphs and apply faceting to organize it by continent.\nHere’s what’s happening:\n\nfacet_wrap('continent') instructs plotnine to create a separate panel for each unique value in the continent column.\nPanels are arranged from left to right, and when they don’t fit on one row, they “wrap” onto the next line.\n\nThe result? A clean, organized set of charts where each panel highlights the GDP-per-capita and life expectancy trends for a specific continent. Faceting is especially helpful when the number of panels is manageable, and it lets us compare trends within each group side by side.\nLet’s take this idea further. What happens to the relationship between GDP per capita and life expectancy over time?\nTry faceting by year instead of continent.See if you can answer these questions\n\nThis exercise offers an incredible opportunity to see how historical events, global growth, and inequality have shaped the world over decades.\n\nWith everything we’ve learned so far, we can summarize the plotnine template as follows::\n\nFaceting is a powerful addition to your visualization toolkit, especially when your data has distinct groups or categories. Whether you’re analyzing trends over continents or time, faceting can make your insights clearer and more impactful.\nSo, go ahead—try faceting your own graphs. You’ll be amazed at what you uncover!\n\n\nVIDEO 13 {greenscreen} ~ 7 min\n\nWe’ve built our chart layer by layer, and now it’s time to refine it for presentation—whether for your boss, a client, or publication. The final touches, like annotations and labels, can make all the difference in ensuring your audience understands your insights clearly.\nLet’s start with some practical data transformations. Instead of showing GDP per capita in raw numbers, wouldn’t it be better to express it in thousands of dollars? Similarly, population is easier to interpret when expressed in millions.\nWith plotnine, we don’t need to preprocess our data for this. You can specify transformations directly in your chart code. For example gdpPercap/1e3 divides GDP per capita by 1,000, and uses scientific notation (1e3) for convenience. Similarly, you can use pop/1e6 to show population in millions.\nTo make our chart clear and professional, we’ll use the labs() function. This function gathers all labels in one place, allowing us to customize:\n\nTitle and subtitle at the top,\nCaption at the bottom,\nLabels for x, y, and any mapped aesthetics, like color or size.\n\nHere’s an example of a polished, annotated chart:\nEach label corresponds to the aesthetics used in the aes() mappings. Make sure all mapped aesthetics are labeled, even if they appear in just one layer.\nNow, let’s make your chart stand out! plotnine offers pre-selected themes that adjust the colors, fonts, and overall style of your plots.\nOne of my favorites is theme_minimal(). It simplifies the design, creating a clean and modern look:\nplotnine offers plenty of built-in themes to match your purpose:\n\ntheme_dark() for a sleek, high-contrast look.\ntheme_linedraw() for a simple, hand-drawn aesthetic.\ntheme_xkcd() for a playful, comic-style appearance.\ntheme_538() for a polished, professional newsroom feel.\n\nThemes contributed by the community can add even more variety. So, explore, experiment, and find the one that best suits your data story.\nCongratulations! You’ve learned about aesthetics, scales, different types of geoms and now you also know how to annotate and apply themes to your visuals to make them more compelling. With these skills, you’re ready to create polished, professional-quality charts that truly stand out.\nGood luck, and we can’t wait to see the insights you’ll uncover!"
  }
]